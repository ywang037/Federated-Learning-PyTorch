{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07d85e14-81ae-4711-821a-a7b012747b8d",
   "metadata": {},
   "source": [
    "# Model selection\n",
    "\n",
    "### CIFAR10 experiments\n",
    "* In [the vanilla FL paper](https://arxiv.org/abs/1602.05629) the author reported that the CNN model used in for learning CIFAR10 data was grabbed from the TensorFlow tutorial.\n",
    "* The exact model borrowed from the [current TensorFlow tutorial](www.tensorflow.org/tutorials/images/cnn) (referred to as the *TF model*) is somewhat different from the one described in the vanilla paper as this *TF model* only has $\\approx1.2\\times10^5$ parameters, which is just one-tenth of the one described in the vanilla paper (has $\\approx 10^6$ parameters). \n",
    "* Howover, this *TF model* is still approximately two times larger than the one used in [PyTorch tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html) (referred to as the *Torch model*) who has $\\approx 0.6\\times 10^5$ parameters. \n",
    "* The implemenation found in [AshwinRJ's repository](https://github.com/AshwinRJ/Federated-Learning-PyTorch) uses the above *Torch model* for experiments with CIFAR10.\n",
    "\n",
    "### MNIST experiments\n",
    "* in [the vanilla FL paper](https://arxiv.org/abs/1602.05629), they describes the MLP (2NN) and CNN used for MNIST experiments as below:\n",
    "\n",
    "> A simple multilayer-perceptron with 2-hidden layers with 200 units each using ReLu activations (199,210 total parameters), which we refer to as the MNIST 2NN.\n",
    "\n",
    "> A CNN with two 5x5 convolution layers (the first with 32 channels, the second with 64, each followed with 2x2 max pooling), a fully connected layer with 512 units and ReLu activation, and a final softmax output layer (1,663,370 total parameters).\n",
    "\n",
    "* The related MLP and CNN implementations shown in AshwinRJ's repository, are different with the above descriptions:\n",
    "    1. AshwinRJ's MLP has only one hidden layer where as the FL uses two.\n",
    "    2. AshwinRJ's CNN has much fewer number of channels in convolutional layers and much fewer units in the fully connected layer.\n",
    "\n",
    "\n",
    "* WY tried to follow the model architecture described in the vanilla paper for the MLP and CNN used for MNIST experiments, but the obtaind model stll has fewer number of parameters.\n",
    "\n",
    "\n",
    "### Rationale\n",
    "* Since our goal is to evaluate FL vs baseline method, not to achieve the best possible accuracy on CIFAR10 learning task, therefore the smaller Torch model is sufficient for our needs, considering that training the TF model may require much longer time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcecc859-40d4-47c0-be52-406562905e2a",
   "metadata": {},
   "source": [
    "### Usable models for CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7572f3d9-6f8d-468d-bcf5-91070b10b205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change workding directory for the convenience of this notebook\n",
    "import os\n",
    "os.chdir('C:/Users/wangyuan/myfl-1/Federated-Learning-PyTorch/src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e86e708-7873-47d8-9af3-60d1503c2eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import models\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c137e1e6-7420-4903-b261-6facc26a59d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch model (original) \thas: 62006\tparameters.\n",
      "Torch model (WY's) \thas: 62006\tparameters.\n",
      "TF model (WY's) \thas: 122570\tparameters.\n"
     ]
    }
   ],
   "source": [
    "# create a model instance for Torch model (copied from the original repository)\n",
    "model_torch_0 = models.CNNCifar()\n",
    "\n",
    "# create a model instance for Torch model (created by WY)\n",
    "model_torch_1 = models.CNNCifarTorch()\n",
    "\n",
    "# create a model instance for Torch model (created by WY)\n",
    "model_tf = models.CNNCifarTf()\n",
    "\n",
    "print(f'Torch model (original) \\thas: {utils.get_count_params(model_torch_0)}\\tparameters.')\n",
    "print(f'Torch model (WY\\'s) \\thas: {utils.get_count_params(model_torch_1)}\\tparameters.')\n",
    "print(f'TF model (WY\\'s) \\thas: {utils.get_count_params(model_tf)}\\tparameters.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2e01b6-28b3-4add-8fc6-e68eefb9f11d",
   "metadata": {},
   "source": [
    "#### Equivalence of Torch models\n",
    "Note that the number of trainable parameters is identical for both Torch models (original) and the one created by WY. Although their model class definition is slightly different, the actual model should be identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "714a1997-fa1d-459b-8435-c00456e1bb4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the Torch model in the original repository\n",
      " CNNCifar(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n",
      "\n",
      "This is the Torch model created by WY\n",
      " CNNCifarTorch(\n",
      "  (conv_layer): Sequential(\n",
      "    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc_layer): Sequential(\n",
      "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=84, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print('This is the Torch model in the original repository\\n', model_torch_0)\n",
    "print('\\nThis is the Torch model created by WY\\n', model_torch_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe251bf-8c9b-466f-b463-05e5b1cdfe47",
   "metadata": {},
   "source": [
    "### Usable models for MNIST\n",
    "#### 2NN comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3cf490c-afb7-492e-bf2d-0ffb069d41bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the 2NN model in the original repository:\n",
      " MLP(\n",
      "  (layer_input): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (layer_hidden): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n",
      "\n",
      "This is the 2NN model created by WY:\n",
      " TwoNN(\n",
      "  (nn_layer): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=100, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=100, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "2NN model for MNIST (original) \thas: 50890\tparameters.\n",
      "2NN model for MNIST (WY's) \thas: 89610\tparameters.\n",
      "2NN model for MNIST (FL paper) \thas: 199210\tparameters.\n"
     ]
    }
   ],
   "source": [
    "# show the model architecture\n",
    "print('This is the 2NN model in the original repository:\\n', models.MLP(28*28,64,10))\n",
    "print('\\nThis is the 2NN model created by WY:\\n', models.TwoNN())\n",
    "\n",
    "# show the number of trainalbe parameters\n",
    "print(f'2NN model for MNIST (original) \\thas: {utils.get_count_params(models.MLP(28*28,64,10))}\\tparameters.')\n",
    "print(f'2NN model for MNIST (WY\\'s) \\thas: {utils.get_count_params(models.TwoNN())}\\tparameters.')\n",
    "print('2NN model for MNIST (FL paper) \\thas: 199210\\tparameters.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bb9750-fdcd-4c71-a642-23f0fd2b48da",
   "metadata": {},
   "source": [
    "#### CNN comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94b24008-5a2e-4c6d-bc4d-2ef6a762fee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the CNN model in the original repository:\n",
      " CNNMnist(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n",
      "\n",
      "This is the CNN model created by WY:\n",
      " CNNMnistWy(\n",
      "  (conv_layer): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc_layer): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "CNN model for MNIST (original) \thas: 22340\tparameters.\n",
      "CNN model for MNIST (WY's) \thas: 583626\tparameters.\n",
      "CNN model for MNIST (FL paper) \thas: 1663370\tparameters.\n"
     ]
    }
   ],
   "source": [
    "# show the model architecture\n",
    "print('This is the CNN model in the original repository:\\n', models.CNNMnist())\n",
    "print('\\nThis is the CNN model created by WY:\\n', models.CNNMnistWy())\n",
    "\n",
    "# show the number of trainalbe parameters\n",
    "print(f'CNN model for MNIST (original) \\thas: {utils.get_count_params(models.CNNMnist())}\\tparameters.')\n",
    "print(f'CNN model for MNIST (WY\\'s) \\thas: {utils.get_count_params(models.CNNMnistWy())}\\tparameters.')\n",
    "print('CNN model for MNIST (FL paper) \\thas: 1663370\\tparameters.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a027a483-a3e6-42cb-85a4-4791315b995b",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
